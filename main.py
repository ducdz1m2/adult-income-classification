import pandas as pd
from sklearn.model_selection import train_test_split
import sys
from preprocessing import PreProcessing
from models.dt import DecisionTree
from models.knn import KNN 
from models.nb import Naive_Bayes_Model
from models.rf import RandomForest_Model
from evaluate import Evaluator
from plot import Plotter

sys.stdout.reconfigure(encoding='utf-8')

# --- CHáº Y CHÆ¯Æ NG TRÃŒNH CHÃNH ---
if __name__ == "__main__":
    # 1ï¸âƒ£ Äá»c dá»¯ liá»‡u
    df = pd.read_csv("data/adult.data", header=None, skipinitialspace=True, encoding='utf-8')


    # 2ï¸âƒ£ Tiá»n xá»­ lÃ½
    pre = PreProcessing(df)
    X, y = pre.process() 
    print(f">>> Dá»¯ liá»‡u sau tiá»n xá»­ lÃ½: X={X.shape}, y={y.shape}")

    # 3ï¸âƒ£ Chia dá»¯ liá»‡u huáº¥n luyá»‡n / kiá»ƒm thá»­
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 4ï¸âƒ£ Huáº¥n luyá»‡n mÃ´ hÃ¬nh 
    tree = DecisionTree(criterion= 'gini',max_depth=10,min_samples_split=2, min_samples_leaf=1)
    tree.train(X_train, y_train)
    print("âœ… Huáº¥n luyá»‡n xong DecisionTreeClassifier.")

    knn = KNN(n_neighbors=11,p=2, weights='uniform')
    knn.train(X_train, y_train)
    print("âœ… Huáº¥n luyá»‡n xong KNeighborsClassifier.")

    
    nb = Naive_Bayes_Model()
    nb.train(X_train, y_train)
    print("Huáº¥n luyá»‡n xong mÃ´ hÃ¬nh NaiveBayesClassifier.")

    rf = RandomForest_Model(n_estimators=200, min_samples_split = 10, min_samples_leaf = 1, max_depth=None, bootstrap=True, random_state=42)
    rf.train(X_train, y_train)
    print("Huáº¥n luyá»‡n xong mÃ´ hÃ¬nh RandomForestClassifier.")

    # 5ï¸âƒ£ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
    evaluator = Evaluator(tree.model, X_test, y_test)
    results = evaluator.evaluate()
    print("\n>>> Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh Decision Tree:")
    for metric, value in results.items():
        print(f"- {metric}: {value:.4f}")


    evaluator_knn = Evaluator(knn.model, X_test, y_test)
    results_knn = evaluator_knn.evaluate()
    print("\n>>> Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ KNN:")
    for metric, value in results_knn.items():
        print(f"- {metric}: {value:.4f}")

    evaluator_nb = Evaluator(nb.model,X_test, y_test)
    results_nb = evaluator_nb.evaluate()
    print("\n Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ Navie Bayes:")
    for metric, value in results_nb.items():
        print(f"- {metric}: {value:.4f}")

    evaluator_rf = Evaluator(rf.model,X_test, y_test)
    results_rf = evaluator_rf.evaluate()
    print("\n Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ Random Forests:")
    for metric, value in results_rf.items():
        print(f"- {metric}: {value:.4f}")

    # 6ï¸âƒ£ Top feature quan trá»ng
    top_n = 15
    feature_names = list(X.columns)  # convert sang list Ä‘á»ƒ plot Ä‘áº¹p
    importances = tree.model.feature_importances_
    sorted_idx = importances.argsort()[-top_n:]
    print("\nTop feature names & importances:")
    for i in sorted_idx:
        print(f"- {feature_names[i]}: {importances[i]:.4f}")

    # 7ï¸âƒ£ Váº½ biá»ƒu Ä‘á»“ feature importance
    plotter = Plotter(tree.model, X_test, y_test)
    plotter.feature_importance(feature_names, top_n=top_n)




# ÄÃ¢y lÃ  tÃªn cÃ¡c cá»™t trong DataFrame Ä‘Ã£ xá»­ lÃ½, bao gá»“m:

# Numeric columns: fnlwgt, age, hours-per-week, capital-gain, capital-loss, education-num

# One-hot columns: native-country_X, workclass_X, occupation_X, relationship_X, education_X, marital-status_X

# Vá»›i one-hot, má»—i giÃ¡ trá»‹ riÃªng biá»‡t trong cá»™t categorical trá»Ÿ thÃ nh má»™t feature. VÃ­ dá»¥:

# workclass_Private = 1 náº¿u ngÆ°á»i Ä‘Ã³ lÃ m Private, 0 náº¿u khÃ´ng.

# marital-status_Married-civ-spouse = 1 náº¿u ngÆ°á»i Ä‘Ã³ married, 0 náº¿u khÃ´ng.

# >>> Dá»¯ liá»‡u sau tiá»n xá»­ lÃ½: X=(30162, 104), y=(30162,)

# === ğŸ” Tuning Decision Tree ===
# Best params: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}
# CV F1: 0.6575396211973845
# CV F1: 0.6575396211973845

# === ğŸ” Tuning KNN ===
# Best params: {'n_neighbors': 11, 'p': 2, 'weights': 'uniform'}
# CV F1: 0.6328032819678856

# === ğŸ” Tuning Naive Bayes ===
# Best params: {'var_smoothing': np.float64(0.1)}
# CV F1: 0.6364977073433581

# === ğŸ” Tuning Random Forest ===
# Best params: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': True}
# CV F1: 0.6907034208522354